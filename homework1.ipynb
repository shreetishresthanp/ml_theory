{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbecffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "p = 100  # dimension\n",
    "n_samples_per_task = 200  # number of samples per task\n",
    "noise_std = 0.1  # standard deviation of Gaussian noise\n",
    "\n",
    "# Generate two tasks\n",
    "# Task 1\n",
    "X1 = np.random.randn(n_samples_per_task, p)  # features from isotropic Gaussian\n",
    "beta1 = np.random.randn(p)  # true parameter vector for task 1\n",
    "y1 = X1 @ beta1 + np.random.normal(0, noise_std, n_samples_per_task)  # labels with noise\n",
    "\n",
    "# Task 2\n",
    "X2 = np.random.randn(n_samples_per_task, p)  # features from isotropic Gaussian\n",
    "beta2 = np.random.randn(p)  # true parameter vector for task 2\n",
    "y2 = X2 @ beta2 + np.random.normal(0, noise_std, n_samples_per_task)  # labels with noise\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Task 1 - Feature matrix shape: {X1.shape}, Label vector shape: {y1.shape}\")\n",
    "print(f\"Task 2 - Feature matrix shape: {X2.shape}, Label vector shape: {y2.shape}\")\n",
    "print(f\"Beta 1 norm: {np.linalg.norm(beta1):.4f}\")\n",
    "print(f\"Beta 2 norm: {np.linalg.norm(beta2):.4f}\")\n",
    "print(f\"Correlation between beta1 and beta2: {np.corrcoef(beta1, beta2)[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment: OLS vs HPS with varying δ\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fixed sample sizes\n",
    "n1 = 200  # samples for task 1\n",
    "n2 = 100  # samples for task 2\n",
    "p = 100   # feature dimension\n",
    "noise_std = 0.1\n",
    "\n",
    "# Generate base parameter vector for task 1\n",
    "beta1_base = np.random.randn(p)\n",
    "\n",
    "# Test data (larger sample for reliable evaluation)\n",
    "n_test = 1000\n",
    "X1_test = np.random.randn(n_test, p)\n",
    "X2_test = np.random.randn(n_test, p)\n",
    "\n",
    "# Vary δ from 0.01 to 1.00\n",
    "deltas = np.linspace(0.01, 1.00, 20)\n",
    "ols_task2_losses = []\n",
    "hps_losses = []\n",
    "\n",
    "for delta in deltas:\n",
    "    # Create beta2 such that ||beta1 - beta2|| = delta\n",
    "    # beta2 = beta1 + delta * normalized_random_vector\n",
    "    random_direction = np.random.randn(p)\n",
    "    random_direction /= np.linalg.norm(random_direction)\n",
    "    beta2 = beta1_base + delta * random_direction\n",
    "    \n",
    "    # Generate training data for both tasks\n",
    "    X1_train = np.random.randn(n1, p)\n",
    "    y1_train = X1_train @ beta1_base + np.random.normal(0, noise_std, n1)\n",
    "    \n",
    "    X2_train = np.random.randn(n2, p)\n",
    "    y2_train = X2_train @ beta2 + np.random.normal(0, noise_std, n2)\n",
    "    \n",
    "    # Test labels for task 2\n",
    "    y2_test = X2_test @ beta2 + np.random.normal(0, noise_std, n_test)\n",
    "    \n",
    "    # 1. OLS Estimator: Train only on task 2\n",
    "    # Solution: beta_hat = (X2^T X2)^{-1} X2^T y2\n",
    "    X2T_X2 = X2_train.T @ X2_train\n",
    "    X2T_y2 = X2_train.T @ y2_train\n",
    "    beta2_ols = np.linalg.solve(X2T_X2 + 1e-8 * np.eye(p), X2T_y2)\n",
    "    \n",
    "    # OLS test loss on task 2\n",
    "    y2_pred_ols = X2_test @ beta2_ols\n",
    "    ols_loss = np.mean((y2_test - y2_pred_ols) ** 2)\n",
    "    ols_task2_losses.append(ols_loss)\n",
    "    \n",
    "    # 2. Hard Parameter Sharing (HPS) Estimator\n",
    "    # Pool data from both tasks and learn a shared representation\n",
    "    # Concatenate training data from both tasks\n",
    "    X_combined = np.vstack([X1_train, X2_train])\n",
    "    y_combined = np.hstack([y1_train, y2_train])\n",
    "    \n",
    "    # Learn shared parameter using all combined data\n",
    "    X_combined_T_X_combined = X_combined.T @ X_combined\n",
    "    X_combined_T_y_combined = X_combined.T @ y_combined\n",
    "    beta_shared = np.linalg.solve(X_combined_T_X_combined + 1e-8 * np.eye(p), X_combined_T_y_combined)\n",
    "    \n",
    "    # HPS test loss on task 2 (using shared representation)\n",
    "    y2_pred_hps = X2_test @ beta_shared\n",
    "    hps_loss = np.mean((y2_test - y2_pred_hps) ** 2)\n",
    "    hps_losses.append(hps_loss)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(deltas, ols_task2_losses, 'o-', label='OLS (Task 2 only)', linewidth=2, markersize=6)\n",
    "plt.plot(deltas, hps_losses, 's-', label='HPS (Hard Parameter Sharing)', linewidth=2, markersize=6)\n",
    "plt.xlabel('δ = ||β₁ - β₂||', fontsize=12)\n",
    "plt.ylabel('Test Loss on Task 2', fontsize=12)\n",
    "plt.title('Transfer Learning: OLS vs HPS Estimators', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"δ range: [{deltas[0]:.2f}, {deltas[-1]:.2f}]\")\n",
    "print(f\"OLS loss range: [{min(ols_task2_losses):.4f}, {max(ols_task2_losses):.4f}]\")\n",
    "print(f\"HPS loss range: [{min(hps_losses):.4f}, {max(hps_losses):.4f}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
